x3_t <- ts(data$x3)
# Benjamin Beckers
# Problem Set 3, Problem 5
# Load packages
install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
data_t <- ts(data)
y_t <- ts(data$y)
x1_t <- ts(data$x1)
x2_t <- ts(data$x2)
x3_t <- ts(data$x3)
y_tlag <- lag(y_t,k=1)
x1_tlag <- lag(x1_t,k=1)
x2_tlag <- lag(x2_t,k=1)
x3_tlag <- lag(x3_t,k=1)
fix(`x3_tlag`)
trash = y[1:149]
trash = y_t[1:149]
fix(trash)
viewData(data)
viewData(data)
y_tlag <- y_t[1:(length(y_t)-1)] # problematic as 0 is inserted instead of trimming the data!
fix(y_tlag)
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x1_t[1:(length(x1_t)-1)]
x3_tlag <- x1_t[1:(length(x1_t)-1)]
x3_t <- x3_t[2:(length(x3_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
y_t <- y_t[2:(length(y_t))]
# Benjamin Beckers
# Problem Set 3, Problem 5
# Load packages
install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
y_t <- data$y
x1_t <- data$x1
x2_t <- data$x2
x3_t <- data$x3
# Lagged variables
y_tlag <- y_t[1:(length(y_t)-1)]
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x1_t[1:(length(x1_t)-1)]
x3_tlag <- x1_t[1:(length(x1_t)-1)]
# Trimmed current variables
y_t <- y_t[2:(length(y_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
x3_t <- x3_t[2:(length(x3_t))]
source('H:/Courses/Econometrics/ps3_p5.R')
ols1 <- lm(y_t ~ 0 + y_tlag+x1_t+x2_t+x3_t)
fix(ols1)
summary(ols1)
summary(ols1)
source('H:/Courses/Econometrics/ps3_p5.R')
# Benjamin Beckers
### Problem Set 3, Problem 5 ###
# Load packages
# install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
y_t <- data$y
x1_t <- data$x1
x2_t <- data$x2
x3_t <- data$x3
# Lagged variables
y_tlag <- y_t[1:(length(y_t)-1)]
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x1_t[1:(length(x1_t)-1)]
x3_tlag <- x1_t[1:(length(x1_t)-1)]
# Trimmed current variables
y_t <- y_t[2:(length(y_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
x3_t <- x3_t[2:(length(x3_t))]
### OLS Regression
ols1 <- lm(y_t ~ 0 + y_tlag+x1_t+x2_t+x3_t)
summary(ols1)
source('H:/Courses/Econometrics/ps3_p5.R')
u_ols = ols$res
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
acf_u_ols <- acf(u_ols)
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
corr(y_t,x1_t)
cor(y_t,x1_t)
source('H:/Courses/Econometrics/ps3_p5.R')
# Benjamin Beckers
### Problem Set 3, Problem 5 ###
# Load packages
# install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
y_t <- data$y
x1_t <- data$x1
x2_t <- data$x2
x3_t <- data$x3
# Lagged variables
y_tlag <- y_t[1:(length(y_t)-1)]
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x1_t[1:(length(x1_t)-1)]
x3_tlag <- x1_t[1:(length(x1_t)-1)]
# Trimmed current variables
y_t <- y_t[2:(length(y_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
x3_t <- x3_t[2:(length(x3_t))]
### OLS Regression
ols <- lm(y_t ~ 0 + y_tlag+x1_t+x2_t+x3_t)
summary(ols)
# Check assumption of uncorrelated error terms u
u_ols = ols$res
acf_u_ols <- acf(u_ols) # Assumption is violated -> OLS biased and inconsistent
### 2SLS-Estimation (as IV Estimation)
cor(y_t,x1_t)
cor(y_t,x2_t)
cor(y_t,x3_t)
cor(x2_t,x3_t)
iv <- lm(y_tlag ~ +x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
source('H:/Courses/Econometrics/ps3_p5.R')
# Benjamin Beckers
### Problem Set 3, Problem 5 ###
# Load packages
# install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
y_t <- data$y
x1_t <- data$x1
x2_t <- data$x2
x3_t <- data$x3
# Lagged variables
y_tlag <- y_t[1:(length(y_t)-1)]
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x1_t[1:(length(x1_t)-1)]
x3_tlag <- x1_t[1:(length(x1_t)-1)]
# Trimmed current variables
y_t <- y_t[2:(length(y_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
x3_t <- x3_t[2:(length(x3_t))]
### OLS Regression
ols <- lm(y_t ~ 0 + y_tlag+x1_t+x2_t+x3_t)
summary(ols)
# Check assumption of uncorrelated error terms u
u_ols <- ols$res
acf_u_ols <- acf(u_ols) # Assumption is violated -> OLS biased and inconsistent
### 2SLS-Estimation (as IV Estimation)
cor(y_t,x1_t)
cor(y_t,x2_t) # use x2_tlag as instrument
cor(y_t,x3_t) # use x3_tlag as instrument
cor(x2_t,x3_t) # correlation between instruments reduces their efficiency
# Auxiliary regression
iv <- lm(y_tlag ~ +x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
source('H:/Courses/Econometrics/ps3_p5.R')
iv <- lm(y_tlag ~ x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
iv <- lm(y_tlag ~ x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
# Benjamin Beckers
### Problem Set 3, Problem 5 ###
# Load packages
# install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
y_t <- data$y
x1_t <- data$x1
x2_t <- data$x2
x3_t <- data$x3
# Lagged variables
y_tlag <- y_t[1:(length(y_t)-1)]
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x1_t[1:(length(x1_t)-1)]
x3_tlag <- x1_t[1:(length(x1_t)-1)]
# Trimmed current variables
y_t <- y_t[2:(length(y_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
x3_t <- x3_t[2:(length(x3_t))]
### OLS Regression
ols <- lm(y_t ~ 0 + y_tlag+x1_t+x2_t+x3_t)
summary(ols)
# Check assumption of uncorrelated error terms u
u_ols <- ols$res
acf_u_ols <- acf(u_ols) # Assumption is violated -> OLS biased and inconsistent
### 2SLS-Estimation (as IV Estimation)
cor(y_t,x1_t)
cor(y_t,x2_t) # use x2_tlag as instrument
cor(y_t,x3_t) # use x3_tlag as instrument
cor(x2_t,x3_t) # correlation between instruments reduces their efficiency
# Auxiliary regression
iv <- lm(y_tlag ~ x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
source('H:/Courses/Econometrics/ps3_p5.R')
iv <- lm(y_tlag ~ 0 + x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
iv <- lm(y_tlag ~ 0 + x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
# Benjamin Beckers
### Problem Set 3, Problem 5 ###
# Load packages
# install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
y_t <- data$y
x1_t <- data$x1
x2_t <- data$x2
x3_t <- data$x3
# Lagged variables
y_tlag <- y_t[1:(length(y_t)-1)]
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x1_t[1:(length(x1_t)-1)]
x3_tlag <- x1_t[1:(length(x1_t)-1)]
# Trimmed current variables
y_t <- y_t[2:(length(y_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
x3_t <- x3_t[2:(length(x3_t))]
### OLS Regression
ols <- lm(y_t ~ 0 + y_tlag+x1_t+x2_t+x3_t)
summary(ols)
# Check assumption of uncorrelated error terms u
u_ols <- ols$res
acf_u_ols <- acf(u_ols) # Assumption is violated -> OLS biased and inconsistent
### 2SLS-Estimation (as IV Estimation)
cor(y_t,x1_t)
cor(y_t,x2_t) # use x2_tlag as instrument
cor(y_t,x3_t) # use x3_tlag as instrument
cor(x2_t,x3_t) # correlation between instruments reduces their efficiency
# Auxiliary regression
iv <- lm(y_tlag ~ 0 + x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
summary(iv)
source('H:/Courses/Econometrics/ps3_p5.R')
# Benjamin Beckers
### Problem Set 3, Problem 5 ###
# Load packages
# install.packages("dynlm")
data <- read.table("H:/Courses/Econometrics/XY.txt", header=T)
# Specify data as time series
y_t <- data$y
x1_t <- data$x1
x2_t <- data$x2
x3_t <- data$x3
# Lagged variables
y_tlag <- y_t[1:(length(y_t)-1)]
x1_tlag <- x1_t[1:(length(x1_t)-1)]
x2_tlag <- x2_t[1:(length(x2_t)-1)]
x3_tlag <- x3_t[1:(length(x3_t)-1)]
# Trimmed current variables
y_t <- y_t[2:(length(y_t))]
x1_t <- x1_t[2:(length(x1_t))]
x2_t <- x2_t[2:(length(x2_t))]
x3_t <- x3_t[2:(length(x3_t))]
### OLS Regression
ols <- lm(y_t ~ 0 + y_tlag+x1_t+x2_t+x3_t)
summary(ols)
# Check assumption of uncorrelated error terms u
u_ols <- ols$res
acf_u_ols <- acf(u_ols) # Assumption is violated -> OLS biased and inconsistent
### 2SLS-Estimation (as IV Estimation)
cor(y_t,x1_t)
cor(y_t,x2_t) # use x2_tlag as instrument
cor(y_t,x3_t) # use x3_tlag as instrument
cor(x2_t,x3_t) # correlation between instruments reduces their efficiency
# Auxiliary regression
iv <- lm(y_tlag ~ 0 + x1_t+x2_t+x3_t+x2_tlag+x3_tlag)
summary(iv)
source('H:/Courses/Econometrics/ps3_p5.R')
y_tlag_hat <- iv$fitted.values
source('H:/Courses/Econometrics/ps3_p5.R')
summary(two_sls)
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
confint(two_sls)
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
source('H:/Courses/Econometrics/ps3_p5.R')
sqrt(3/8)+sqrt(1/24)
(sqrt(3/8)+sqrt(1/24))^2
2*(sqrt(3/8)+sqrt(1/24))
2*(sqrt(3/2)+sqrt(1/6))
(sqrt(3/2)+sqrt(1/6))
sqrt(8/3)
(2-sqrt(2))^2
x1 = 0.5
x1 = 0.5
x2 = (1-x1)/(1-0.5*x1)
x1 = 0.5
x1 = 0.25
x2 = (1-x1)/(1-0.5*x1)
x1 = 0.75
x2 = (1-x1)/(1-0.5*x1)
2-sqrt(2)
2-sqrt(2)
1-(2-sqrt(2))
1-sqrt(2)/2
2*(sqrt(2)-1)
sqrt(2)/2
curve((1-x)/(1-x/2), from = 0 , to = 1, main = "Production Possibility Frontier", ylab = "f(x)=(1-x)/(1-x/2)")
curve(-x, add = TRUE, col = "red")
curve((1-x)/(1-x/2), from = 0 , to = 1, main = "Production Possibility Frontier", ylab = "f(x)=(1-x)/(1-x/2)")
curve(-x, from = 0 , to = 1, add = TRUE, col = "red")
curve((1-x)/(1-x/2), from = 0 , to = 1, main = "Production Possibility Frontier", ylab = "f(x)=(1-x)/(1-x/2)")
curve(1-x, from = 0 , to = 1, add = TRUE, col = "red")
sqrt(1/2)
sqrt(3/4)
-0.5
sqrt(3/4)-0.5
(3-sqrt(3))/2
(3-sqrt(3))/(sqrt(3)+1)
sqrt(2)-1
2-sqrt(2)
setwd('H:/Projects/Forecast Instability around Bubbles/Code')
WS <- c(ls())
rm(list=WS)
target = "IPT"
horizon = 13
filename1 = paste('h:/git/bubblesbreakdowns/results/',target,' hor ',toString(horizon),' rolling_ARX.RData',sep="")
filename2 = paste('h:/git/bubblesbreakdowns/results/',target,' hor ',toString(horizon),' rolling_AR.RData',sep="")
###### Load forecasts ######
load(filename1)
load(filename2)
###### Forecast evaluation ######
yeval = set.rt$IPT
yeval = as.matrix(1200*log(yeval[(1+horizon):length(yeval)]/yeval[1:(length(yeval)-horizon)])/horizon,nrow=1)
yeval = yeval[!is.na(yeval)]
# Remove last #horizon forecasts for which no evaluation is possible
vint.dates = vint.dates[1:(length(vint.dates)-horizon-1)]
Neval = length(vint.dates)
forecast.ar = as.numeric(forecast.ar[1:Neval,,drop=F])
forecast.arx = forecast.arx[,1:Neval,drop=F]
msr.ar = as.numeric(msr.ar[1:Neval,,drop=F])
msr.arx = msr.arx[,1:Neval,drop=F]
forecast.arx = rbind(t(forecast.ar),forecast.arx)
row.names(forecast.arx)[1]="AR"
msr.arx = rbind(t(msr.ar),msr.arx)
row.names(msr.arx)[1]="AR"
Nmodels = nrow(forecast.arx)
yeval = as.matrix(yeval[(length(yeval)-Neval+1):length(yeval)],nrow=1)
# Timeline of forecast margins
timeline=seq((1983+7/12),(2013+9/12),by=(1/12))
# Forecast errors
error.arx = forecast.arx-matrix(1,Nmodels,1)%*%t(yeval)
sqerror.arx = error.arx^2
###### Test for Equal Predictive Accuracy ######
source('Clark_West_Test/f_Clark_West_Test.R')
source('Clark_West_Test/f_Newey_West_vector.R')
Clark_West_Test = matrix(NA,nrow=Nmodels-1,ncol=2)
for (n in 2:Nmodels){
trash = f_Clark_West_Test(sqerror.arx[1,],sqerror.arx[n,],forecast.arx[1,],forecast.arx[n,],horizon)
Clark_West_Test[(n-1),] = cbind(trash[[1]],trash[[2]])
}
###### Dating of Forecast Breakdowns ######
# Surprise loss
SL.arx = sqerror.arx-msr.arx
# Variances
sqerror.arx.demeaned=sqerror.arx-rowMeans(sqerror.arx)%*%matrix(1,nrow=1,ncol=Neval)
SLL.arx=cov(t(sqerror.arx.demeaned))
SLL.arx=diag(SLL.arx)
# Parameters
lambda=2/3*(max.obs/Neval)
# HAC variance estimator of demeaned surprise losses
bw=0#floor(Neval^(1/3))# rounded down
SLL.arx = matrix(0,nrow=1,ncol=Nmodels)
for (n in 1:Nmodels){
hacest<-function(sqerror.demeaned,Neval,bw){
SLL = 0
for (j in 1:bw){
Gamma = t(sqerror.demeaned[(1+j):Neval])%*%sqerror.demeaned[1:(Neval-j)]/Neval
SLL = SLL+2*(1-j/(bw+1))*Gamma
}
SLL = SLL+t(sqerror.demeaned)%*%sqerror.demeaned/Neval
}
SLL.arx=apply(sqerror.arx.demeaned,1,hacest,Neval,bw)
}
# Variance estimator out-of-sample losses
sigma2 = lambda*SLL.arx
# regression of SL on themselves
pmax = 12
source(paste(DirCode,'/olsself.R',sep=''))
surprise.BIC = matrix(NA,nrow=pmax,ncol=Nmodels)
for (p in 1:pmax){
surprise.res=apply(SL.arx,1,olsself,p)
surprise.BIC[p,]=sapply(surprise.res,function(x) x$BIC)
}
surprise.pstar = apply(surprise.BIC, 2, which.min)
for (n in 1:Nmodels){
surprise.res[[n]]=olsself(SL.arx[n,],surprise.pstar[n])
}
surprise.coef=sapply(surprise.res,function(x) x$b)
surprise.regressors=sapply(surprise.res,function(x) x$Z)
surprise.resid=sapply(surprise.res,function(x) x$res)
surprise.fit=sapply(surprise.res,function(x) x$yfit)
# Confidence Interval
source(paste(DirCode,'/FB_Wald_CI.R',sep=''))
surprise.CI = list()
for (n in 1:Nmodels){
trash = FB_Wald_CI(surprise.res[[n]]$Z,surprise.res[[n]]$res,surprise.res[[n]]$b,Neval,max.obs,sigma2[n],1,"rolling",bw,0.05)
surprise.CI[[n]]=trash$SLfitCI
}
###### Plots ######
dataplot = cbind(matrix(0,nrow=Neval-surprise.pstar[1],ncol=1),surprise.fit[[1]],surprise.CI[[1]])
matplot(timeline[(surprise.pstar[1]+1):Neval],dataplot,type="l")
## Extract data for plots
SL.arx.fit = matrix(NA,nrow=Nmodels,ncol=Neval)
SL.arx.CI = matrix(NA,nrow=Nmodels,ncol=Neval)
for (n in 1:Nmodels){
trashfit = surprise.fit[[n]]
trashCI = surprise.CI[[n]]
SL.arx.fit[n,] = rbind(matrix(NA,nrow=surprise.pstar[n],ncol=1),trashfit)
SL.arx.CI[n,] = rbind(matrix(NA,nrow=surprise.pstar[n],ncol=1),trashCI)
}
######  Best model by rank ######
fcastranks = apply(sqerror.arx,2,rank)
fcastranksavg = apply(fcastranks,1,mean)
# Best model by rank
which.min(fcastranksavg)
# Forecast breakdowns
modbreak=(SL.arx.CI>0)*1
Nmodbreak=rowSums(modbreak,na.rm=T)
cor.test(fcastranksavg,Nmodbreak)
library("BMA", lib.loc="~/R/win-library/3.1")
library("knitr", lib.loc="~/R/win-library/3.1")
detach("package:BMA", unload=TRUE)
library("BMA", lib.loc="~/R/win-library/3.1")
library("MCS", lib.loc="~/R/win-library/3.1")
MCS <- MCSprocedure(Loss = sqerror.arx, alpha = 0.05, B = 5000, statistic = "Tmax")
MCS <- MCSprocedure(Loss = sqerror.arx, alpha = 0.05, B = 100, statistic = "Tmax")
MCS <- MCSprocedure(Loss = sqerror.arx, alpha = 0.05, B = 1000, statistic = "Tmax")
MCS <- MCSprocedure(Loss = sqerror.arx, alpha = 0.05, B = 100, statistic = "Tmax")
MCS <- MCSprocedure(Loss = sqerror.arx, alpha = 0.05, B = 5000, statistic = "Tmax")
help MCSprocedure
MCSprocedure?
? MCSprocedure
data(Loss)
View(Loss)
View(Loss)
Loss[1,]
Loss[,1]
view(Loss[,])
View(Loss[,])
View(t(sqerror.arx))
MCS <- MCSprocedure(Loss = t(sqerror.arx), alpha = 0.05, B = 5000, statistic = "Tmax")
